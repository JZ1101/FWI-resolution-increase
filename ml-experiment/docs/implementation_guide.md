Great. I will explore machine learning and deep learning approaches for downscaling ERA5 Fire Weather Index data from \~25 km resolution to ≤1 km resolution using only available reanalysis and land cover data. The focus will be on techniques that do not require true 1 km ground-truth FWI targets, while ensuring positive-definite outputs, model transferability to other regions, and physically reasonable results.

I'll summarize viable models, training strategies, input feature engineering, evaluation metrics, and validation techniques suitable for this geospatial super-resolution task.


# Downscaling ERA5 Fire Weather Index from 25 km to 1 km Using Machine Learning

## Introduction and Data Overview

The goal is to increase the spatial resolution of the ERA5 Fire Weather Index (FWI) from its native \~25 km grid to a finer resolution (on the order of 1 km) for a test region (e.g. Portugal). Achieving this requires leveraging available meteorological reanalysis data at higher resolutions and applying machine learning (ML) or deep learning techniques for spatial downscaling. We have several datasets at our disposal:

* **ERA5 Reanalysis (\~25 km)** – Provides daily atmospheric parameters (wind components at 10 m, 2 m dewpoint temperature, 2 m temperature, total precipitation) as well as a derived daily FWI on a \~0.25° grid.
* **ERA5-Land (\~10 km)** – A land-focused reanalysis at \~0.1° resolution, giving higher-resolution fields for near-surface variables (temperature, dewpoint, wind, precipitation, etc.).
* **UERRA European Reanalysis (\~5 km)** – A high-resolution regional reanalysis for Europe (\~5.5 km grid) that offers 10 m wind speed, 2 m relative humidity, 2 m temperature, and precipitation, capturing finer-scale terrain and land-surface effects.
* **ESA WorldCover Land Cover (10 m)** – High-resolution land cover classifications (11 classes) that can be aggregated to \~1 km, to account for surface characteristics (forest, grassland, urban areas, water bodies, etc.) in the downscaling process.
* **Topographic Data (e.g. DEM)** – Although not explicitly listed, high-resolution elevation data are implicitly available (and could be derived from coordinates). Elevation will be important for capturing orographic effects on weather.

All these data are open-access (Creative Commons licensed), though retrieval requires accounts on their respective portals. They come in structured formats (often NetCDF or GRIB for reanalyses), and a preliminary task is to convert them into a common grid or tabular format for ease of modeling. We will likely extract all datasets to the same geographic sub-region (Portugal) and align them on a common high-resolution grid (e.g. a 0.01° lat-lon grid \~1 km). Ensuring consistent grid alignment and time synchronization (daily values from 2000–2019) is a crucial preprocessing step.

**Challenge – Lack of 1 km FWI Ground Truth:** A key problem is that we do not have observed or reanalysis FWI at 1 km to directly train a supervised model. FWI is normally computed from coarser data; it’s defined by a formula (the Canadian FWI System) that uses daily noon temperature, relative humidity, wind speed, and 24‑hour precipitation as inputs. Therefore, any downscaling method must handle the absence of a direct high-resolution target. We need to rely on proxies or physically derived targets for training, and enforce consistency such that the downscaled FWI is physically plausible (e.g. non-negative and within a realistic range).

**Objectives and Constraints:** The downscaling method should produce positive-definite FWI values (FWI is an index of fire danger that should never be negative). It must also introduce realistic fine-scale spatial variability while remaining consistent with the coarse-scale FWI. In practice, this means if we aggregate the 1 km outputs back up to 25 km, they should correlate well with (or equal) the original ERA5 FWI. We will also evaluate whether the high-resolution patterns seem reasonable: e.g. smoother fields without pixelation artifacts, and spatial patterns that correlate with known drivers like topography and land cover (for instance, higher FWI in drier interior regions or lower FWI over water bodies or mountains if appropriate). Moreover, the approach should be transferable beyond Portugal – the model should generalize to other regions with minimal tuning, meaning we emphasize physically relevant features rather than region-specific quirks.

## Downscaling Approach Overview

We consider two complementary approaches to downscale FWI: **(1) Physically-informed downscaling using the FWI formula with high-resolution inputs**, and **(2) Data-driven ML/DL super-resolution using coarse-to-fine mapping**. These approaches are not mutually exclusive – in fact, a hybrid strategy could combine physical modeling with ML adjustments. Below we outline each approach:

### 1. Physically-Informed Downscaling (Deterministic)

In this approach, we attempt to compute FWI at \~1 km resolution by feeding high-resolution weather inputs into the standard FWI calculation. Since FWI is computed from daily weather variables, the steps would be:

* **High-Res Meteorological Inputs:** Use ERA5-Land (10 km) and UERRA (5 km) data to derive 1 km fields for temperature, humidity, wind, and precipitation. Standard spatial interpolation (e.g. bilinear or spline) can downscale the coarse fields to 1 km, but simple interpolation won’t capture fine detail. Therefore, we introduce adjustments using topography and land cover:

  * *Temperature:* Apply elevation-based lapse rate corrections. For example, the 2 m temperature from ERA5-Land (10 km) can be downscaled by applying a lapse rate (\~-6.5°C per 1000 m or a regionally fitted value) using a 1 km DEM to adjust temperatures in each 1 km cell according to altitude differences. This accounts for local cooling in high terrain (important in Portugal’s hills vs. coast).
  * *Relative Humidity:* Derive RH from 2 m dewpoint and temperature. After temperature adjustment, recompute RH at 1 km (since humidity is very sensitive to temperature). Also consider altitude effects (higher elevation tends to cooler, potentially more humid conditions if not cloud-limited).
  * *Wind:* Use the 10 m wind components from ERA5-Land or UERRA. Downscale by accounting for terrain slope and exposure – e.g., increase wind speeds on ridges (exposed areas) and decrease in valleys (sheltering effect). One could use a high-res roughness or topographic exposure index as a guide for wind adjustment (this could be a simple parametric model or an ML model trained on known fine-scale winds).
  * *Precipitation:* Disaggregate daily precipitation using known high-res climatology. Rainfall is highly variable; a common approach is to distribute coarse precipitation into finer scales using elevation (orographic enhancement) and perhaps convective variability. For instance, use a higher resolution dataset or climatology (if available) as a scaling map. In absence of that, one might downscale by assuming uniform distribution or use statistical methods like stochastic rainfall downscaling for finer texture, while preserving the coarse daily total.

* **Land Cover Influence:** Land cover doesn’t explicitly enter the FWI formula, but it can indirectly affect micro-climate (e.g., urban heat island or forest cooling due to evapotranspiration). We can incorporate land cover by adjusting temperature and humidity locally (for example, urban areas might be slightly warmer/drier on hot days, vegetated areas cooler/moister). Another role of land cover is to mask out irrelevant areas: e.g., FWI over water bodies or urban cores might not be meaningful. We could set FWI to 0 or a very low value in water pixels since open water cannot have a wildfire. However, for continuity, it might be better to still compute it but note these areas in post-processing.

* **Compute FWI at 1 km:** Using the downscaled inputs, we run the standard Canadian FWI calculation at each 1 km grid cell for each day. This involves computing the intermediate codes (Fine Fuel Moisture Code, Duff Moisture Code, etc.) and then the FWI itself, using the same algorithm used for ERA5 FWI. The algorithm has memory (e.g., Drought Code carries over day to day), so we must propagate the previous day’s moisture codes when moving sequentially through time at each pixel. We would initialize these codes at the start of the season (e.g. spring) using either standard defaults or by interpolating the coarse codes if available.

* **Result:** This yields a physically downscaled FWI field at 1 km that, by construction, is non-negative (the FWI formula yields zero or positive values) and responds to local terrain and land surface differences. We expect this physically downscaled product to have more spatial detail; however, it might not perfectly match the coarse ERA5 FWI when aggregated. For example, simply interpolating and lapse-rate-correcting temperature could introduce biases. Also, errors in downscaled precipitation or wind could propagate to FWI. Therefore, this method might need calibration: comparing the aggregated 1 km FWI back to the original 25 km FWI and removing any systematic bias (e.g., via a regression or bias correction model).

**Pros:** Maintains physical consistency and leverages known formulas; ensures positivity and realistic responses (e.g. a day with zero coarse precipitation remains zero precipitation in each fine cell, preventing spurious wet/dry patterns). It’s also transparent and easily generalizable (the procedure uses global physical principles like adiabatic lapse rates).

**Cons:** The finest detail is limited by what we include in the physical model. Pure interpolation plus simple corrections might not capture all sub-grid variability. This is where a machine learning approach can supplement by learning complex relationships or error patterns.

### 2. Machine Learning–Based Downscaling (Super-Resolution)

In this approach, we treat the coarse-to-fine mapping as a learnable function, using machine learning or deep learning to predict 1 km FWI patterns from coarse inputs and high-resolution auxiliary data. This is essentially a **statistical downscaling** or **super-resolution** task. We outline the strategy:

* **Feature Construction:** We assemble a rich set of input features for the ML model, at 1 km resolution:

  * *Coarse FWI and Weather:* The ERA5 coarse FWI itself can be used as a feature – essentially a baseline value that needs refinement. We can also include the coarse ERA5 meteorological inputs (T, RH, wind, precip) interpolated to 1 km as additional channels, to inform the model about the large-scale weather conditions.
  * *Higher-Res Reanalysis:* Include fields from ERA5-Land or UERRA at 5–10 km, downsampled/interpolated to 1 km. These carry more spatial detail than the 25 km data. For example, UERRA 5 km temperature and humidity will reflect meso-scale microclimates; ERA5-Land precipitation might distinguish coastal vs inland rainfall better than ERA5.
  * *Terrain and Land Surface:* High-resolution DEM (elevation), slope, aspect, and land cover class or fractions. These static features help the model learn how FWI should vary within a coarse grid cell. For instance, higher elevations within a coarse cell might often have lower temperature and higher humidity, thus slightly lower FWI than the coarse average – the model can learn this relationship. Land cover could differentiate forested sub-areas (which might retain moisture) from open areas.
  * *Derived Indices:* We can include indices like NDVI (vegetation index) or land surface temperature from satellites, if available, as they can indicate surface moisture and heat which correlate with fire weather conditions. For example, a high NDVI (dense vegetation) area might have different fine-scale fire weather characteristics than an urban area, even under the same coarse cell weather. These additional predictors were successfully used in downscaling air temperature to 1 km in other studies.

* **Model Choice:** We have options ranging from simpler ML models (e.g., random forest, XGBoost) to deep learning models (Convolutional Neural Networks or hybrid architectures). Given the spatial nature and the need for smooth, coherent output, a CNN-based approach is very appealing:

  * *CNN Super-Resolution:* We can frame the coarse FWI map as a low-resolution image and aim to produce a high-resolution image. The model would take a patch of the coarse grid (plus high-res feature maps) and output a refined patch at higher resolution. Architectures like **U-Net** (which combines coarse context with fine detail through skip connections) or **ResNet/EDSR** (Enhanced Deep Super-Resolution network with residual blocks) are suitable. In fact, recent research shows that deep super-resolution models such as VDSR and EDSR significantly outperform a baseline CNN (SRCNN) when downscaling climate data, achieving higher PSNR and better structural similarity in downscaled fields. These models, by using deep residual layers, can learn to add the subtle spatial details (“high-frequency” components) missing in the coarse data while preserving the large-scale patterns.
  * *Gradient Boosted Trees / Random Forest:* As an alternative or complement, we could train a regression model that for each 1 km cell predicts the *increment* or *ratio* of FWI relative to the coarse cell mean, using the features at that cell. For example, a gradient boosting model (XGBoost) could learn that within a given coarse cell, a subcell’s elevation and land cover will adjust the FWI up or down from the coarse value. XGBoost has been successfully used for climate downscaling tasks (e.g. downscaling ERA5 temperature to 1 km) with good accuracy. The advantage of tree-based models is that they are easier to interpret and handle nonlinear relationships well with less training data; the disadvantage is that they predict each point independently, potentially leading to less spatially smooth outputs compared to CNNs (though we can impose some averaging or smoothness post-hoc).

* **Training Strategy (Dealing with No Direct 1 km Target):** Since we lack actual 1 km FWI observations, we must get creative to train the model:

  * *Synthetic Targets via Higher-Res Data:* We can leverage the 5 km UERRA reanalysis as a pseudo “ground truth” at a scale finer than 25 km. For instance, compute the FWI using UERRA’s 5 km weather data (using the same FWI formula). This gives us a 5 km FWI field for Portugal for the period 2000–2019. We know UERRA’s FWI should be closer to reality at small scales than ERA5’s 25 km FWI. Now, we can create training pairs by **upscaling UERRA (5 km) FWI to 25 km** (spatial averaging) and treating the result as a coarse input, with the original 5 km FWI as the target. Essentially, this trains the model to **downscale from 25 km to 5 km** using real data. Once this 5 km model is learned, we can apply it to go further down to 1 km by providing 1 km-resolution features (land cover, elevation) – possibly in multiple steps (25 km → 5 km, then 5 km → 1 km). If 5 km training is successful, the same model architecture can be extended, though the 1 km step may require assuming similar relationships hold from 5→1 as from 25→5. We might also directly train for 25→1 km by degrading a high-res dataset: for example, take the computed 5 km FWI as *true*, aggregate it to 25 km, then train the model to recover 5 km from 25 km (with features available at 1 km or 5 km resolution). This supervised learning approach uses the reanalysis as a substitute for observations.
  * *Spatial Cross-Validation with Observations:* If any weather stations in Portugal have calculated FWI (some fire agencies compute daily FWI at station points), we could use those to validate or even train point-specific models. However, station coverage is sparse; instead, we follow the approach of validating against station weather (as was done in downscaling temperature). That is, ensure that if we sample our downscaled FWI at the locations of meteorological stations, the values correspond well to the FWI computed from the station’s weather readings. This provides an independent check on realism.
  * *Unsupervised or Constrained Learning:* For a deep learning model, another clever training method is **self-supervised downscaling**: treat the coarse ERA5 FWI as input and require the CNN to produce a higher-res output that, when **re-aggregated** to coarse scale, matches the original. We could add a loss term that penalizes the difference between the mean of the predicted 1 km cells (over each coarse cell) and the ERA5 coarse FWI (ensuring fidelity to known large-scale values). Meanwhile, another part of the loss can encourage the output to resemble patterns from a proxy high-res field (e.g., match the gradient or distribution of the 5 km FWI, or even use adversarial training where a discriminator ensures the fine output “looks” statistically like real fine-scale FWI fields). This is more complex to implement but would allow training without explicit 1 km targets.
  * In practice, we will likely use the **synthetic supervised training with UERRA 5 km data** as it is straightforward: it provides a real physical target at a smaller scale. Indeed, a similar downscaling study for temperature created training data by downsampling high-res reanalysis and training super-resolution CNNs, which proved effective.

* **Loss Functions and Constraints:** For a deep learning model, we will use appropriate loss functions. A mean squared error (MSE) loss on the fine-scale predictions against the target (e.g. 5 km “truth”) is standard. We may also incorporate spatial structure metrics (like SSIM – structural similarity – to encourage realistic texture). Crucially, we must enforce non-negativity of FWI. We can achieve this by using a **non-linear activation** (e.g. ReLU) at the output layer of the network, which forces all predictions to ≥0. For tree-based models, we can simply clip any negative predictions to zero after the fact (though with proper training and target being non-negative, they should rarely predict negative). We will also monitor that the distribution of predicted FWI values stays within plausible bounds (for example, if coarse FWI is say 20, it would be unrealistic for a downscaling to produce 1 km values of 100; extreme values usually occur only when coarse is also extreme, due to the nature of FWI formula).

* **Spatial Smoothness and Coherence:** One risk with ML downscaling is producing a “patchwork” look (especially with pointwise regressors like XGBoost). To mitigate this, our feature set can include neighboring information (for instance, a convolutional filter inherently uses surrounding context). If using XGBoost, we could include aggregated features from a neighborhood (like average elevation in a 5 km radius, etc.) to allow some spatial context. However, a CNN or ConvLSTM approach naturally yields spatially coherent outputs because of the convolutional receptive field. In fact, a recent study on fire danger downscaling used a ConvLSTM (convolutional LSTM) to ensure **spatio-temporally consistent** predictions, treating the problem in both space and time context. We could draw inspiration from that by perhaps introducing temporal context (e.g., include previous day’s FWI or moisture code as an input feature to the model) so that day-to-day variations at a location are smooth and realistic.

**Pros:** A data-driven model can learn complex nonlinear influences that are hard to capture manually. For example, interactions like “FWI spikes more on leeward side of a mountain due to drying foehn winds” could be learned if the data support it, without explicitly coding that rule. ML can also blend many predictors (satellite data, land cover, etc.) to optimally improve the estimate. Previous work has shown that ML and especially deep learning can significantly improve downscaling accuracy for climate variables, achieving RMSE on the order of 1°C for 1 km temperature downscaling – we aim for similar improvements in capturing FWI variability.

**Cons:** It requires a robust training strategy to compensate for lack of direct high-res FWI truth. We must be cautious of overfitting to Portugal region; if the model learns very local patterns that don’t generalize (say a particular bias over one mountain range), it might not work elsewhere. We address this with careful validation (e.g., using a *leave-one-area-out* validation where we train on part of Portugal and test on a held-out sub-region or on Spain, mimicking deploying the model in a new region). Additionally, ML models are less interpretable than a physical formula – but we can use explainability tools (e.g., feature importance in XGBoost or saliency maps in CNNs) to verify that the model is using sensible inputs (like elevation, humidity, etc.) rather than latching onto spurious correlations.

## Implementation Plan

**Data Preparation:** We will first download and subset all relevant data to the Portugal region. Then, resample everything to a common 1 km grid (most likely a regular lat-lon grid covering Portugal). We’ll produce a daily time series of features for 2000–2019. Due to the volume (20 years of daily data at 1 km can be large), we might handle one year at a time or use clever data structures (NetCDF stacking) so we can feed the ML model efficiently. Conversion to tabular format for tree models or to image patches for CNNs will be done as needed:

* Ensure that each 1 km grid cell has attributes: coarse ERA5 FWI (value is same for all 1 km cells within a 25 km block), coarse ERA5 weather, interpolated ERA5-Land weather, UERRA 5 km weather (maybe downsampled a bit), land cover class, elevation, etc., for each day.
* Compute the target for training (e.g., UERRA-derived FWI at 5 km or if we choose some other proxy). This involves running the FWI calculation on UERRA data – we can use an existing library or script for the Canadian FWI system to get daily FWI from those inputs.

**Model Training:** Depending on the approach:

* If using **CNN/Deep Learning**, we will likely patch the data into smaller regions for training because an input of full Portugal at 1 km (\~N x M grid) might be too large for memory if we include multiple features. One approach is to use sliding window patches of, say, 64×64 km (covering a few coarse cells each) as training samples. Each sample’s low-res input will be the corresponding ERA5 FWI patch (maybe 2×2 coarse cells if 25 km each), and high-res target is the 64×64 1 km FWI patch from pseudo-truth. We will augment the input with high-res feature maps (e.g., elevation map of that patch, etc.) as additional channels. We train the CNN to minimize loss between its output and the target patch. Techniques like residual learning (have the network predict the *difference* between coarse and fine FWI) can help convergence. We will be careful to hold out some portion of data for validation – likely by date (e.g., all days of year 2019) and by sub-region (maybe set aside NW Portugal as a spatial test) to ensure generalization.
* If using **XGBoost/Random Forest**, we will create a large table where each row is a 1 km cell on a certain day, with columns for features and the target fine FWI. We can then train the model to predict target from features. To avoid data leakage and overweighting certain days, we will sample or weight the data. One could also train separate models for different seasons if relationships differ (FWI dynamics in summer vs winter). Given the data size (potentially millions of samples), XGBoost can still handle it, but we might do a smarter sampling (or use GPU training).

**Ensuring Positive Outputs:** For XGBoost/RF, we can impose a rule-based post-process to cap minimum at 0 (and realistically, FWI rarely needs a cap on max, but if the model overshoots extremely, we can also cap at some high percentile value). For the CNN, as mentioned, a final ReLU ensures outputs are ≥0. Empirically, because our training targets (FWI values) are all ≥0, the model is unlikely to produce negatives if it fits well, but the safety net will be in place.

**Iteration and Refinement:** After initial training, we’ll evaluate:

* Does the model maintain coarse consistency? (Check that averaging the 1 km predictions per 25 km grid matches the ERA5 value. If not exactly, are the errors unbiased or do we need to explicitly enforce it via a constraint or fine-tune with a loss on the aggregate?)
* Is the spatial detail plausible? (We expect smoother transitions, not a checkerboard. If using a CNN with upsampling layers, sometimes checkerboard artifacts appear – using bilinear upsampling + convolution or sub-pixel convolution can mitigate that.)
* Are extreme FWI events localized correctly? (If coarse FWI was very high over a region, the fine version should show high values predominantly in areas that likely would have higher risk – e.g., maybe lower humidity valleys or regions with more flammable land cover – rather than uniformly high everywhere. The auxiliary features should help the model concentrate the high index in the most vulnerable sub-areas.)
* Feature importance or sensitivity: we can check, for instance, in XGBoost which features were most used. We expect elevation, land cover, fine-scale temperature and humidity to rank high. If something unexpected dominates (e.g., an artifact of how data were prepared), we may adjust.

If issues are found, we’ll adjust the approach. For example, if we find the model produces too much variation (very noisy), we might add a smoothing regularization or train it to predict a slightly blurred version of truth then sharpen it – or simply average the output a bit. If the model isn’t adding much detail (output looks too similar to interpolated coarse field), we might increase network capacity or include more informative features (like including not just current day but also previous day’s FWI or soil moisture estimates to give it memory of dryness).

## Validation and Evaluation

Once we have a downscaled FWI dataset for Portugal at 1 km, we will validate it in several ways:

* **Comparison to Original Coarse FWI:** Aggregation of the fine grid to 25 km should ideally reproduce the ERA5 FWI. We’ll compute correlation and bias for each day’s aggregated value vs. original. We expect near-perfect agreement if the method explicitly conserved it, or at least high correlation with minimal bias if the model was trained well. Any systematic bias (e.g., always overshoots by +1) can be corrected easily.

* **Spatial Patterns and Smoothness:** Visual inspection of a few example days (e.g., a day with high fire weather, a day with low fire weather) will be done. We will look for realistic features: e.g., along a coastline, perhaps the fine FWI shows lower values right near the ocean (due to maritime humidity) and higher inland; in mountainous areas, perhaps valleys show different FWI than peaks. The transitions should be smooth and physically sensible (no pixel-level erratic jumps). If available, we could overlay actual fire occurrence data (burned area or active fire detections) to see if high FWI hotspots align better at 1 km – this is a qualitative check.

* **Quantitative Skill against Independent Data:** Although FWI itself isn’t directly observed, we can use station weather data from Portugal’s meteorological agency. At each station, compute FWI from observed T, RH, wind, precip. Then compare those point values to the nearest grid cell in our downscaled FWI vs. the coarse ERA5 FWI. The downscaled version should show improved accuracy (lower error) because it has incorporated local elevation and land effects. We can report metrics like RMSE and correlation of FWI at station locations. (This is analogous to what was done in downscaling temperature, where downscaled 1 km temperature had \~0.98°C RMSE against stations, much better than raw model data.)

* **Cross-Region Generalization:** To test transferability, we could apply the Portugal-trained model to a neighboring region (say Spain or France) and see if the output still looks reasonable. Since the model uses physical features, it should, for example, handle the Pyrenees or coastal Mediterranean climates without recalibration. If we detect problems (like systematic bias in a different climate), we may need to include a more diverse training sample or incorporate normalization (ensuring the model knows, for example, the range of input values it might see elsewhere).

* **Comparing to Alternative Downscaling or Higher-Res Model:** If any higher-resolution FWI or fire danger product exists (perhaps a \~1 km gridded FWI from a national service, or even our own physically downscaled FWI from approach 1), we can compare our ML output to that. High correlation and similar distribution would build confidence. For example, if we do physical downscaling and ML downscaling separately, their results should be in broad agreement – if not, investigating differences might highlight if the ML model found some pattern the physical method missed (or vice-versa).

Throughout validation, we will document qualitative and quantitative findings to demonstrate that the high-resolution FWI data is **reasonable**. For instance, we expect the fine FWI to be **smooth** except where real geographical discontinuities exist (coastline, abrupt elevation changes) – any blocky artifacts would be flagged as a problem. We also anticipate that the downscaled FWI will have **higher variance** spatially (since at 1 km we can capture extremes that were averaged out at 25 km) but **no bias** in the mean. This means if coarse FWI was, say, 10 uniformly over a cell, the new 1 km values might range from 5 to 15 in different parts, but averaging to \~10. Such variation should correlate with inputs (e.g. the spot with 15 might be an especially hot/dry lowland area that day).

## Conclusion

To summarize, our plan combines domain knowledge of fire weather with modern machine learning to achieve high-resolution FWI estimates:

* We will utilize the Canadian FWI system formula and high-res data to generate a physically-consistent baseline.
* We will then train machine learning models (such as CNN-based super-resolution networks or gradient-boosted trees) to enhance and refine the spatial details, using reanalysis-based pseudo-observations for supervised learning.
* The approach will enforce physical constraints (like non-negativity and preservation of large-scale means) so that results remain realistic.
* By incorporating topography, land cover, and other fine-scale predictors, the downscaled FWI should reflect local variability (e.g. microclimates in complex terrain) that the coarse ERA5 misses, while still aligning with the known coarse values.
* We will thoroughly validate the outcome in Portugal and design the method to generalize elsewhere with minimal adjustment, thus providing a blueprint for **transferrable downscaling** of fire weather indices.

If successful, this method will produce daily FWI maps at \~1 km resolution that can support local fire risk assessment and management, bridging the gap between global reanalysis and local needs. It showcases how **machine learning** can complement physical models in climate and weather science, yielding enhanced resolution without running a full dynamical model at 1 km (which would be computationally prohibitive). By carefully leveraging available data and ensuring consistency, we avoid mere “black-box” predictions – instead we create a data-driven yet physically aware downscaling pipeline, as befits a collaboration between ML experts and weather experts (which we aim to be, sans buzzwords!).


**Testing and Evaluation:**
Different location performance, transferability to other regions, and physically reasonable results.
Visual inspection of downscaled FWI maps, comparison to coarse ERA5 FWI, and validation against station data.
**Sources:**

* Canadian FWI inputs and system description.
* Downscaling of climate variables with ML (features and algorithms).
* Deep learning super-resolution results for climate data.
* ERA5-based FWI data and related resources.
