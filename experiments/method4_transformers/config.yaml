# Method 4: Vision Transformers for Downscaling

name: "method4_transformers"
description: "Deep learning with vision transformers"

base_config: "../../configs/base.yaml"

model:
  name: "transformer"
  upscale_factor: 4
  architecture: "vision_transformer"
  
  # Transformer architecture
  patch_size: 2
  embed_dim: 256
  num_heads: 8
  num_layers: 6
  mlp_ratio: 4.0
  
  # Training
  dropout: 0.1
  attention_dropout: 0.1
  
training:
  required: true
  epochs: 100
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.0001
  scheduler: "cosine"
  warmup_epochs: 10
  
  # Data augmentation
  augmentation:
    random_crop: false
    horizontal_flip: false  # Not appropriate for geographic data
    
  # Loss
  loss_function: "mse"
  use_perceptual_loss: false
  
# Specific for transformers
inference:
  use_sliding_window: true
  window_overlap: 0.25